--- 

dir:
    order: 1
    # collapsible: false
    text: Artificial Intelligence
index: false
title: Artificial Intelligence


---



## **AI发展简史与技术原理**

人工智能（AI）作为一门交叉学科，经历了多个发展阶段：

- **1950s-1980s：符号主义AI**：使用规则和知识图谱来模拟推理，代表系统如Expert System。

- **1980s-2010s：统计学习与神经网络**：神经网络、多层感知器、支持向量机等方法成为主流，尤其是在图像识别、语音识别等领域取得突破。

- **2017至今：大模型时代（LLM）**：Transformer架构的提出（Google的"Attention is All You Need"论文），开启了预训练+微调范式，ChatGPT、Claude、Gemini 等多模态大模型横空出世


::: info AI发展里程碑
- **1950s**：图灵测试提出，AI概念诞生  
- **1980s**：专家系统兴起（如MYCIN医疗诊断）  
- **1997年**：IBM深蓝击败国际象棋冠军  
- **2012年**：AlexNet引爆深度学习革命  
- **2017年**：Transformer架构诞生（GPT、BERT的基础）  
- **2023年**：ChatGPT推动大模型普及  
:::


### **现代AI核心技术**
- **深度学习**：基于神经网络的表征学习  
- **Transformer**：自注意力机制解决长序列依赖  
- **扩散模型**：Stable Diffusion等图像生成基础  
- **强化学习**：AlphaGo、自动驾驶决策核心  

::: important 核心原理：Transformer架构
- **Encoder-Decoder结构**：主要用于翻译任务。
- **Self-Attention机制**：使得模型能同时关注输入的不同部分，提高理解能力。
- **预训练+微调**：大模型先在海量数据上无监督预训练，然后针对特定任务进行微调，提升效果
:::


#### **关键公式示例（注意力机制）**  
$$
\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

---

### 主流大模型对比

| **模型**       | **公司**    | **特点**                          | **适用场景**               |
|----------------|------------|-----------------------------------|--------------------------|
| GPT-4          | OpenAI     | 多模态、强推理能力                | 通用问答、代码生成        |
| Claude 3       | Anthropic  | 长上下文（200K tokens）           | 文档分析、法律文本        |
| Gemini 1.5     | Google     | 多模态交互最优                    | 跨模态搜索、视频理解      |
| LLaMA 3        | Meta       | 开源可商用（8B-70B参数）          | 企业私有化部署            |
| Mistral 7B     | Mistral AI | 轻量级高效                        | 边缘设备、快速推理        |

::: tip 模型架构对比
- **自回归模型**（GPT系列）：逐token生成，适合文本创作  
- **双向编码模型**（BERT）：上下文理解，适合分类任务  
- **混合架构**（T5）：编码器-解码器，适合翻译/摘要  
:::

---


### **开发者工具推荐**
| **工具**          | **用途**                      | **链接**                    |
|-------------------|-----------------------------|----------------------------|
| Hugging Face      | 模型库与数据集                | [huggingface.co](https://huggingface.co) |
| Ollama            | 本地运行大模型                | [ollama.ai](https://ollama.ai) |
| LangChain         | AI应用开发框架                | [langchain.com](https://langchain.com) |
| LM Studio         | 本地GUI模型管理               | [lmstudio.ai](https://lmstudio.ai) |

---



### **Token**

Token是AI大模型处理文本的基本单位，它代表了模型输入输出的最小语义片段。

在自然语言处理(NLP)中，Token可以是一个单词、子词(subword)或符号

- OpenAI Token计算器：https://platform.openai.com/tokenizer
- 第三方Token计算：https://tiktoken.aigc2d.com/

::: info Token的成本计算方式

```
总成本 = (输入Token数 + 输出Token数) × 每Token单价
```
影响成本的因素
- **模型类型**：不同模型(如GPT-3.5与GPT-4)的Token单价不同
- **API提供商**：各平台定价策略有差异
- **上下文长度**：长上下文需要更多计算资源
- **请求频率**：批量处理可能享受折扣
:::

典型定价示例(以OpenAI为例，2023年数据)
| 模型 | 输入单价(每1K Tokens) | 输出单价(每1K Tokens) |
|------|----------------------|----------------------|
| GPT-3.5 Turbo | $0.0015 | $0.002 |
| GPT-4 | $0.03 | $0.06 |
| GPT-4-32k | $0.06 | $0.12 |

实际成本计算案例-假设使用GPT-4模型：
- 输入：1,500 Tokens
- 输出：800 Tokens
- 计算：
  ```
  输入成本 = ceil(1500/1000) × $0.03 = 2 × $0.03 = $0.06
  输出成本 = ceil(800/1000) × $0.06 = 1 × $0.06 = $0.06
  总成本 = $0.06 + $0.06 = $0.12
  ```

::: tip Token成本优化技巧

#### 1. 输入优化策略

**精简提示词(Prompt Pruning)**
- 删除不必要的礼貌用语和冗余信息
- 示例优化：
  ```
  差: "你好，请问你能帮我总结一下这篇文章吗？非常感谢你的帮助！"
  优: "总结这篇文章："
  ```

**上下文压缩技术**
- 使用向量检索只提取相关上下文
- 实现摘要式上下文而非完整文本
- 采用递归检索策略

**结构化输入**
- 使用JSON等结构化格式提高信息密度
- 示例：
  ```json
  {"task":"summary","text":"..."}
  ```

#### 2. 输出优化策略

**限制输出长度**
- 设置max_tokens参数
- 明确要求简洁回答
  ```
  "用不超过50字回答：..."
  ```

**输出格式控制**
- 要求特定格式(如列表、表格)提高信息密度
- 示例：
  ```
  "以表格形式列出优缺点，每点不超过5个词"
  ```

**流式处理**
- 对长内容分块处理，及时中断不需要的部分

#### 3. 系统级优化

**缓存机制**
- 缓存常见问题的标准回答
- 实现Token-aware缓存策略

**模型选择策略**
- 简单任务使用低成本模型
- 复杂任务才用高端模型

**批量处理**
- 合并多个请求为批量调用
- 实现请求队列和聚合

### 4. 监控与分析

**Token使用监控**
- 实现实时Token计数器
- 设置预算警报阈值

**成本分析仪表盘**
- 按功能/部门/用户分析Token消耗
- 识别高成本热点

**A/B测试**
- 比较不同提示词的Token效率
- 优化高频率查询
:::



---



### **Prompt Engineering**

提示词工程（Prompt Engineering），简单来说，就是输入给 AI 的指令

::: info 大模型提示词分类与设计指南

#### 基于角色的分类（核心分类）

1. 用户提示词(User Prompt)
- **定义**：用户直接输入的请求或指令
- **功能**：明确告诉AI"做什么"

示例："总结这篇文章的核心观点"

2. 系统提示词(System Prompt)
- **定义**：设定AI行为规则的隐藏指令
- **功能**：定义AI的角色定位和能力边界
- **关键要素**：
  - 角色身份（如"资深营养师"）
  - 专业领域（如"擅长制定减肥食谱"）
  - 回答风格（如"使用通俗易懂的语言"）
  - 限制条件（如"不提供医疗诊断"）

3. 助手提示词(Assistant Prompt)
- **定义**：AI的响应内容
- **功能**：在多轮对话中形成上下文记忆
- **应用**：可预设引导性回复塑造对话方向

#### 基于功能的分类

1. **指令型**：明确任务（"将以下文本翻译成法语"）
2. **对话型**：自然交流（"你对区块链技术怎么看？"）
3. **创意型**：内容生成（"创作一首关于秋天的俳句"）
4. **角色扮演型**：特定身份（"作为莎士比亚评论这首诗"）
5. **少样本学习型**：示例引导（提供2-3个示例规范输出格式）

#### 基于复杂度的分类

1. **简单型**：单一指令（"解释量子计算"）
2. **复合型**：多重要求（"分析代码+找错+改进建议"）
3. **链式型**：分步执行（首先生成大纲→再扩展内容）
4. **模板型**：结构化变量（"作为{领域}专家回答{问题}"）

:::


1. **角色定义**：系统提示词决定AI的专业性和边界
2. **明确度**：用户提示词越具体，输出质量越高
3. **上下文管理**：合理利用助手提示词引导对话
4. **复杂度匹配**：根据需求选择适当提示词结构


---



### **微调(Fine-tuning)**

微调技术分类：
- **全参数微调（Full Finetuning）**：调优全部模型参数，成本高但适应性强。
- **参数高效微调（PEFT）**：如LoRA、QLoRA，只调整少量参数，部署灵活。
- **指令微调（SFT）**：结合人类标注数据，让模型更符合期望指令。
- **RLHF（基于人类反馈的强化学习）**：OpenAI使用于ChatGPT的重要优化方法。
---

**全参数微调**：更新所有权重，需大量计算资源  
  ```python
  model.train()
  for batch in dataloader:
      outputs = model(**batch)
      loss = outputs.loss
      loss.backward()
      optimizer.step()
  ```
**高效微调方法**：  
  - **LoRA**：低秩适配（仅训练新增的小矩阵）  
  - **Adapter**：插入小型网络模块  
  - **QLoRA**：4bit量化+LoRA，显存需求降低70%  


---



## **核心扩展技术解析**

### **Function Calling**
- **作用**：让大模型触发外部工具（如API、数据库）  
- **示例流程**：  
  ```python
  tools = [{
      "type": "function",
      "function": {
          "name": "get_weather",
          "parameters": {"location": "string"}
      }
  }]
  response = client.chat.completions.create(
      model="gpt-4",
      messages=[{"role": "user", "content": "北京天气如何？"}],
      tools=tools
  )
  ```

---

### **MCP(模型上下文协议)**

**MCP（模型上下文协议）**  
- **架构设计**：标准化接口实现AI与外部系统交互  
- **典型应用**：  
  - 百度地图POI数据调用：整合实时人流热力图生成旅游路线  
  - 实现步骤：  
1. 部署MCP Agent策略  
2. 配置API端点与查询条件  



---

### **RAG(检索增强生成)**
- **架构**：  
  ```mermaid
  graph LR
    A[用户问题] --> B[向量数据库检索]
    B --> C[相关文档片段]
    C --> D[大模型生成答案]
  ```
- **实现工具**：  
  - **LlamaIndex**：文档索引与检索  
  - **FAISS**：高效向量搜索库  

---


## 大模型接入方式

- 云服务

- 本地部署（私有部署）

### AI应用平台

- [阿里云百炼](https://bailian.console.aliyun.com/console?tab=model#/model-market)

### AI软件客户端

cherry studio

cursor/trae...

### 程序调用

模型服务灵积（Dashscope）：https://www.aliyun.com/product/dashscope



---

## **AI应用场景及未来趋势**
**1. 代码辅助**
- **GitHub Copilot**：实时代码补全  
- **CodeLlama**：开源代码生成模型  

**2. 智能问答系统**
- **RAG+GPT**：企业知识库问答  
- **LangChain**：构建AI Agent流水线  

**3. 计算机视觉**
- **YOLOv9**：实时目标检测  
- **SAM**：Meta图像分割一切模型  

**4. 语音交互**
- **Whisper**：语音转录（支持100+语言）  
- **VALL-E**：微软高保真语音克隆  

---


::: info 未来趋势与挑战

- **多模态统一模型**：图像、语音、视频、文本全面融合。
- **Agent化发展**：AI不只是工具，而是自治行动体。
- **个性化模型微调工具普及**：人人可微调小型个人助手。
- **AI+IoT、边缘AI部署加速**：设备端推理、低功耗大模型落地。

---

- **小型化**：1B参数模型达到70B模型能力（如Phi-3）  
- **多模态**：文本/图像/视频/3D统一处理  
- **安全风险**：幻觉（Hallucination）缓解与对齐（Alignment）  
:::


**行动建议**：  

1. 掌握RAG+Function Calling构建企业级应用  
2. 关注开源模型（如Llama 3）的垂直领域微调  















